# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,kaldi_ark --valid_shape_file exp/asr_stats_raw_en_bpe5000_sp/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_samba_raw_en_bpe5000_sp --config conf/tuning/train_asr_samba.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000_sp/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/wav.scp,speech,kaldi_ark --train_shape_file exp/asr_stats_raw_en_bpe5000_sp/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000_sp/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000_sp/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Mon Aug 18 10:16:09 UTC 2025
#
/home/asr/.conda/envs/espnet/bin/python3 /espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,kaldi_ark --valid_shape_file exp/asr_stats_raw_en_bpe5000_sp/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_samba_raw_en_bpe5000_sp --config conf/tuning/train_asr_samba.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000_sp/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/wav.scp,speech,kaldi_ark --train_shape_file exp/asr_stats_raw_en_bpe5000_sp/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000_sp/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000_sp/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[lotus-sys-gpu-33-17] 2025-08-18 10:16:19,928 (asr:540) INFO: Vocabulary size: 5000
[lotus-sys-gpu-33-17] 2025-08-18 10:16:21,051 (abs_task:1400) INFO: pytorch.version=2.6.0+cu126, cuda.available=True, cudnn.version=90501, cudnn.benchmark=True, cudnn.deterministic=False
[lotus-sys-gpu-33-17] 2025-08-18 10:16:21,054 (abs_task:1401) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 20], num_mask=1, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=1, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000_sp/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): SambaASREncoder(
    (embed): Conv2dSubsamplingForSamba(
      (conv): Sequential(
        (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (3): ReLU()
      )
      (out): Linear(in_features=10240, out_features=512, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (pos_enc): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (mamba_blocks): ModuleList(
      (0-5): 6 x MambaBlock(
        (in_proj): Linear(in_features=512, out_features=2048, bias=False)
        (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
        (act): SiLU()
        (x_proj): Linear(in_features=1024, out_features=64, bias=False)
        (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=512, bias=False)
      )
    )
    (layer_norms): ModuleList(
      (0-5): 6 x LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    )
    (final_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): SambaASRDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 512)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (decoders): ModuleList(
      (0-3): 4 x MambaCrossBlock(
        (self_mamba): MambaBlock(
          (in_proj): Linear(in_features=512, out_features=2048, bias=False)
          (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
          (act): SiLU()
          (x_proj): Linear(in_features=1024, out_features=64, bias=False)
          (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=512, bias=False)
        )
        (cross_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (cross_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (ffn): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (output_layer): Linear(in_features=512, out_features=5000, bias=True)
    (after_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=512, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 44.87 M
    Number of trainable parameters: 44.87 M (100.0%)
    Size: 179.47 MB
    Type: torch.float32
[lotus-sys-gpu-33-17] 2025-08-18 10:16:21,054 (abs_task:1404) INFO: Optimizer:
AdamW (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.98]
    capturable: False
    differentiable: False
    eps: 1e-06
    foreach: None
    fused: None
    initial_lr: 0.0002
    lr: 1.0000000000000001e-07
    maximize: False
    weight_decay: 0.01
)
[lotus-sys-gpu-33-17] 2025-08-18 10:16:21,054 (abs_task:1405) INFO: Scheduler: WarmupLR(warmup_steps=2000)
[lotus-sys-gpu-33-17] 2025-08-18 10:16:21,055 (abs_task:1414) INFO: Saving the configuration in exp/asr_train_asr_samba_raw_en_bpe5000_sp/config.yaml
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,171 (abs_task:1828) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=29187, batch_bins=1000000, sort_in_batch=descending, sort_batch=descending)
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,177 (abs_task:1829) INFO: [train] mini-batch sizes summary: N-batch=29187, mean=2.9, min=1, max=18
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,246 (read_text:31) INFO: keys_to_load is not None, only loading 85617 keys from dump/raw/train_clean_100_sp/text
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,341 (asr:512) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,349 (abs_task:1853) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train_clean_100_sp/wav.scp", "type": "kaldi_ark"}
  text: {"path": "dump/raw/train_clean_100_sp/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f9805c9fe50>)
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,386 (abs_task:1828) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=1094, batch_bins=1000000, sort_in_batch=descending, sort_batch=descending)
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,386 (abs_task:1829) INFO: [valid] mini-batch sizes summary: N-batch=1094, mean=5.1, min=1, max=22
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,390 (read_text:31) INFO: keys_to_load is not None, only loading 5551 keys from dump/raw/dev/text
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,395 (asr:512) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,395 (abs_task:1853) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "kaldi_ark"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f9805ccd000>)
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,403 (abs_task:1828) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=5551, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000_sp/valid/speech_shape, 
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,403 (abs_task:1829) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,407 (read_text:31) INFO: keys_to_load is not None, only loading 3 keys from dump/raw/dev/text
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,410 (asr:512) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,411 (abs_task:1853) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "kaldi_ark"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f9805cce0e0>)
/espnet/espnet2/train/trainer.py:219: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
[lotus-sys-gpu-33-17] 2025-08-18 10:16:22,472 (trainer:336) INFO: 1/50epoch started
Failed to import Flash Attention, using ESPnet default: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /home/asr/.conda/envs/espnet/lib/python3.10/site-packages/flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so)
/espnet/espnet2/train/trainer.py:638: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(
/espnet/espnet2/asr/espnet_model.py:402: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(self.autocast_frontend, dtype=autocast_type):
[lotus-sys-gpu-33-17] 2025-08-18 10:21:10,971 (trainer:811) INFO: 1epoch:train:1-100batch: iter_time=9.116e-04, forward_time=0.333, loss_ctc=1.057e+03, loss_att=324.522, acc=0.003, loss=690.794, backward_time=2.522, grad_norm=4.174e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.005, optim0_lr0=5.150e-06, train_time=2.885
Loss: tensor(2532.3569, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(3409.0833, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(2496.0337, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(3046.9158, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(2655.9006, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(1714.7786, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(3352.6890, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(3344.6729, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(1901.4672, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(3041.1167, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(2639.8091, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(2878.9248, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(3104.4351, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(554.8362, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(3316.2456, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(3123.1387, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(2155.0728, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(2783.8943, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(3202.5879, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(2727.5146, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(2360.3679, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(2028.2764, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(2328.6440, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(2370.3567, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(2057.9897, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(2049.3962, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(2323.3708, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(2186.7002, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(1995.0859, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(1838.9349, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(1872.1073, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(1486.1970, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(1367.6812, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(1649.3174, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(1328.3524, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(1267.2305, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(1174.9097, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(1254.9332, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(1323.2164, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(751.6400, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(645.7195, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(898.2659, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(733.3386, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(775.1000, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(718.6165, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(581.9817, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(625.9249, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(557.0747, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(611.7782, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(440.2095, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(486.3212, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(235.2901, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(445.3846, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(439.0009, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(383.4690, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(441.8056, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(393.4986, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(149.9747, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(351.6192, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(497.8420, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(400.0966, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(107.9790, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(474.2794, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(369.3893, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(326.3254, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(374.5548, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(480.3814, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(346.9120, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(475.2698, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(526.7022, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(548.0613, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(269.1702, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(376.2565, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(369.0265, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(399.5556, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(406.3173, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(394.5843, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(359.3981, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(366.1771, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(359.2537, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(446.8990, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(373.8622, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(442.6426, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(436.1173, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(356.3956, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(428.4793, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(336.0719, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(424.4809, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(498.3107, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(371.8805, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(230.7717, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(444.8925, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(440.6221, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(477.9104, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(407.2043, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(342.6192, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(498.0373, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(414.9451, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(437.8154, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(364.3838, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(411.5009, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(490.4496, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(436.3647, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(397.4907, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(370.1532, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(241.6359, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(398.1252, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(540.3241, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(354.1401, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(367.1483, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(334.1390, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(310.7242, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(492.1346, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(451.0687, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(396.6116, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(332.5662, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(402.2146, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(147.1104, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(112.0193, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(411.5692, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(407.8871, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(441.0297, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(173.6570, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(385.6613, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(283.5139, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(339.4500, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(522.8206, device='cuda:0', grad_fn=<DivBackward0>)
Loss: [lotus-sys-gpu-33-17] 2025-08-18 10:25:44,864 (trainer:811) INFO: 1epoch:train:101-200batch: iter_time=1.351e-04, forward_time=0.322, loss_ctc=322.061, loss_att=281.792, acc=0.044, loss=301.927, backward_time=2.390, grad_norm=157.350, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.003, optim0_lr0=1.515e-05, train_time=2.739
tensor(346.0190, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(358.5764, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(378.6023, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(324.4683, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(296.3850, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(423.6150, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(344.0413, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(407.8098, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(422.0869, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(392.6728, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(401.9795, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(293.8904, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(297.9621, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(163.3493, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(398.7011, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(294.8668, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(333.4089, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(304.7005, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(306.2826, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(407.2888, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(369.0624, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(304.1452, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(334.5249, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(384.5295, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(383.5580, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(405.2653, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(433.6745, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(413.7826, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(392.2804, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(357.5408, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(436.6385, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(278.2648, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(467.2837, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(351.4515, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(363.8456, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(403.9550, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(364.5275, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(351.3032, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(395.6096, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(362.6546, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(131.3613, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(436.4807, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(363.7581, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(380.3204, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(166.0105, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(427.1174, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(395.0204, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(295.5077, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(401.0604, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(221.1700, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(125.0752, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(345.4174, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(320.2874, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(441.1404, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(428.1575, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(307.3782, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(370.7686, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(105.6691, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(408.5761, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(314.6652, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(412.3825, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(354.8943, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(143.5045, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(425.8961, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(414.8723, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(432.6526, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(263.4891, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(351.3064, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(312.7394, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(319.8940, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(309.0317, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(314.4883, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(408.9812, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(359.3836, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(210.3670, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(307.5545, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(258.2041, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(307.3461, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(294.5304, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(503.0257, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(425.9892, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(304.0192, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(431.3882, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(360.6166, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(307.3527, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(415.8168, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(378.2714, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(318.3308, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(342.6445, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(162.8916, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(315.0411, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(341.9316, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(317.1174, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(417.4571, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(383.8912, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(382.2570, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(461.5241, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(277.2460, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(472.6269, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(226.0649, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(336.3654, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(323.3792, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(380.6203, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(332.7250, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(389.1010, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(403.0735, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(454.1094, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(458.4588, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(314.0590, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(302.3900, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(306.4000, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(336.2695, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(411.2866, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(375.7712, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(315.6498, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(374.4474, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(315.1148, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(357.5060, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(384.3824, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(308.5057, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(318.8280, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(475.7173, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(297.0816, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(240.5218, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(235.4109, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(173.0203, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(411.3292, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(306.1677, device='cuda:0', grad_fn=<DivBackward0>)
Loss: [lotus-sys-gpu-33-17] 2025-08-18 10:30:40,451 (trainer:811) INFO: 1epoch:train:201-300batch: iter_time=1.367e-04, forward_time=0.396, loss_ctc=330.618, loss_att=281.424, acc=0.052, loss=306.021, backward_time=2.454, grad_norm=179.751, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.030, optim0_lr0=2.515e-05, train_time=2.955
tensor(338.7473, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(249.4951, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(419.6697, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(392.0344, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(354.8415, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(335.3020, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(330.4219, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(366.5595, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(426.9460, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(320.2304, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(327.7283, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(283.6177, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(334.3774, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(386.8076, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(383.3394, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(358.7715, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(357.9764, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(236.2419, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(381.7922, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(365.7141, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(358.0046, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(346.9633, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(354.6899, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(420.8639, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(430.1842, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(387.9765, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(303.7733, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(356.4868, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(290.8945, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(312.9723, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(393.1078, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(385.0474, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(351.8777, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(367.6149, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(339.6951, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(333.0471, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(307.6307, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(326.9023, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(390.1077, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(287.3469, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(393.9743, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(272.8439, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(293.6114, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(434.7358, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(130.0480, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(324.6203, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(165.6460, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(248.9462, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(300.7513, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(316.1456, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(337.0757, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(381.3437, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(350.3387, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(334.8533, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(380.2333, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(311.3868, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(281.8939, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(352.0017, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(415.2719, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(343.9608, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(215.9545, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(417.1172, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(366.5895, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(305.1542, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(361.8707, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(368.7192, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(244.5716, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(386.3695, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(324.4271, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(332.5414, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(183.3756, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(303.4880, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(331.5044, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(317.6931, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(306.4829, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(344.5276, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(363.5222, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(248.2805, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(459.3164, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(293.7027, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(358.2498, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(398.7004, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(351.0034, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(294.2458, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(326.5645, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(148.7367, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(256.2496, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(311.1441, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(335.1578, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(250.1249, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(292.0527, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(357.3335, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(282.1329, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(345.1895, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(318.4673, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(290.2683, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(322.7846, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(258.8748, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(350.0188, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(333.2278, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(449.5280, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(403.1370, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(369.9273, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(316.7368, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(219.4375, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(332.6290, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(378.2783, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(349.1758, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(358.2067, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(328.4851, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(298.2008, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(330.1375, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(313.6614, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(328.2421, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(390.6111, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(324.1882, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(292.0189, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(332.0804, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(457.2149, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(332.9605, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(228.6103, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(276.7064, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(305.2634, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(325.1595, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(299.8605, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(435.7915, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(205.7555, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(365.1313, device='cuda:0', grad_fn=<DivBackward0>)
Loss: [lotus-sys-gpu-33-17] 2025-08-18 10:35:22,142 (trainer:811) INFO: 1epoch:train:301-400batch: iter_time=1.412e-04, forward_time=0.326, loss_ctc=315.778, loss_att=268.546, acc=0.068, loss=292.162, backward_time=2.459, grad_norm=167.955, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.003, optim0_lr0=3.515e-05, train_time=2.818
[lotus-sys-gpu-33-17] 2025-08-18 10:40:03,534 (trainer:811) INFO: 1epoch:train:401-500batch: iter_time=1.347e-04, forward_time=0.310, loss_ctc=292.780, loss_att=245.755, acc=0.077, loss=269.267, backward_time=2.469, grad_norm=152.642, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.003, optim0_lr0=4.515e-05, train_time=2.814
tensor(371.3979, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(357.6212, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(278.9545, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(373.0549, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(373.8585, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(329.3300, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(360.9366, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(288.2265, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(284.3772, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(373.9902, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(334.8058, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(333.5770, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(332.0485, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(435.0739, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(236.8975, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(359.7358, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(295.6032, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(387.6933, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(412.2381, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(333.7734, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(202.8031, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(292.9088, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(449.9280, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(289.2235, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(280.5806, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(313.3163, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(338.4370, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(386.2617, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(281.6739, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(312.0581, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(378.5827, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(257.2499, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(351.8329, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(291.6690, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(287.9759, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(295.0294, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(321.4353, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(348.1706, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(331.3528, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(345.9194, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(312.5432, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(267.4944, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(358.1230, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(389.6539, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(343.9738, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(352.2487, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(297.1291, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(69.0644, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(363.6257, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(294.9967, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(412.3122, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(300.1009, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(447.4982, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(286.0328, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(173.3251, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(281.1507, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(335.4721, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(321.4670, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(323.7167, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(450.2783, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(297.6405, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(418.5908, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(383.3278, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(302.0307, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(278.8335, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(305.8983, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(407.2371, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(348.4434, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(196.1412, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(219.6529, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(338.8900, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(274.1558, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(322.9605, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(323.0960, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(293.5984, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(239.6877, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(347.4262, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(323.1646, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(226.5038, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(251.7722, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(428.3650, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(295.9927, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(323.5138, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(357.7647, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(369.6682, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(282.4216, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(315.8470, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(283.4113, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(356.6058, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(242.2055, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(107.3615, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(382.8876, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(400.3821, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(311.5323, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(240.3667, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(295.0916, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(359.8951, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(350.6450, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(272.2422, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(176.3893, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(323.3262, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(299.4242, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(369.3107, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(446.2508, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(149.7931, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(301.8439, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(289.9865, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(282.8300, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(250.5767, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(381.6595, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(221.5398, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(344.9510, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(355.5684, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(254.7537, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(291.0277, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(332.4229, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(240.5827, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(98.9064, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(353.9540, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(334.2057, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(293.1952, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(354.6656, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(391.9090, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(272.1182, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(338.5289, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(280.0143, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(347.7533, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(212.0019, device='cuda:0', grad_fn=<DivBackward0>)
Loss: [lotus-sys-gpu-33-17] 2025-08-18 10:44:28,245 (trainer:811) INFO: 1epoch:train:501-600batch: iter_time=1.361e-04, forward_time=0.300, loss_ctc=279.034, loss_att=230.874, acc=0.089, loss=254.954, backward_time=2.307, grad_norm=164.212, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.003, optim0_lr0=5.515e-05, train_time=2.647
tensor(283.3181, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(324.7492, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(354.2974, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(324.1245, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(251.8839, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(390.0206, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(337.0885, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(244.9196, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(342.7435, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(115.3114, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(344.1822, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(375.8672, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(319.3875, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(332.8817, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(79.9453, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(227.0931, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(256.5304, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(242.7274, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(218.3844, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(287.0925, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(328.1933, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(309.9598, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(211.4660, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(211.5967, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(284.1517, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(366.3055, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(340.7529, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(299.7626, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(113.8265, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(232.3528, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(480.6873, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(318.7261, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(315.0275, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(451.2741, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(381.2742, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(348.8584, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(309.8777, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(329.2798, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(325.1715, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(300.6199, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(300.5732, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(403.4358, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(380.8659, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(358.2341, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(203.1051, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(359.0044, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(318.0235, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(158.4632, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(320.3702, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(350.7944, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(391.2352, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(336.9762, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(344.5551, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(155.9406, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(238.2255, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(269.0044, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(193.1069, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(341.3866, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(343.7388, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(273.4576, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(372.5917, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(375.7665, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(300.2205, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(200.6696, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(342.6878, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(247.0613, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(304.1900, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(338.3542, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(448.2304, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(353.7896, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(241.5397, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(319.5312, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(289.6133, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(296.3512, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(356.6463, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(304.1936, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(192.0719, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(257.0104, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(306.4753, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(134.6837, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(284.6422, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(233.0840, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(326.1779, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(369.6929, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(338.4569, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(218.2094, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(301.3752, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(314.5109, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(359.0187, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(314.9237, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(294.6739, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(70.7884, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(285.5654, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(209.9219, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(335.6115, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(340.2296, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(237.1989, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(267.4828, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(282.5448, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(238.1785, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(330.7181, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(292.2535, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(355.2372, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(412.5159, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(317.5853, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(301.4626, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(377.1451, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(350.5699, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(371.3611, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(360.8007, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(426.2321, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(319.3523, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(286.7517, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(332.8531, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(294.0553, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(283.8677, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(247.9042, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(351.6705, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(285.6912, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(356.6349, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(230.6084, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(392.3409, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(363.2385, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(296.0056, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(309.1479, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(303.8400, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(376.3720, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(140.1245, device='cuda:0', grad_fn=<DivBackward0>)
Loss: [lotus-sys-gpu-33-17] 2025-08-18 10:48:57,858 (trainer:811) INFO: 1epoch:train:601-700batch: iter_time=1.323e-04, forward_time=0.299, loss_ctc=284.508, loss_att=233.039, acc=0.094, loss=258.774, backward_time=2.357, grad_norm=164.463, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.003, optim0_lr0=6.515e-05, train_time=2.696
tensor(277.5860, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(307.6994, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(380.6157, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(342.9703, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(308.9109, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(302.0184, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(306.1931, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(321.4465, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(324.2734, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(446.4089, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(294.4877, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(353.6834, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(268.1201, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(329.2070, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(329.8049, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(336.4164, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(311.0687, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(367.3505, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(296.4396, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(319.9196, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(362.7852, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(384.2599, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(258.0838, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(385.0875, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(363.5368, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(356.9656, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(406.6857, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(140.6155, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(83.6264, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(345.1294, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(348.9424, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(247.1620, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(252.3707, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(313.6411, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(264.3624, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(369.6426, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(404.7661, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(294.1145, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(325.3083, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(309.9913, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(300.6754, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(367.6153, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(315.3494, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(306.5105, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(249.5622, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(313.4637, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(175.1532, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(338.7106, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(317.4661, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(335.3035, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(298.7261, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(305.0446, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(121.8085, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(176.6365, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(308.6473, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(344.6118, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(286.3760, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(330.5653, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(335.9990, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(390.7071, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(231.0283, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(276.1115, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(278.0585, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(284.5439, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(266.1337, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(168.9012, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(391.7361, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(263.5799, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(380.9793, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(271.3926, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(220.9784, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(338.9633, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(342.9193, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(406.9250, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(398.0306, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(361.2693, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(259.5334, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(312.5546, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(291.1625, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(372.9774, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(352.9892, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(322.1265, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(275.5887, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(320.0211, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(238.4938, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(361.4654, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(300.6538, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(368.6783, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(340.2390, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(267.3676, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(356.0265, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(335.0814, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(454.0585, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(333.4276, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(221.2205, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(120.7625, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(307.1737, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(324.2056, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(354.1172, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(337.6119, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(339.1339, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(295.4575, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(358.6396, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(244.7963, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(269.2629, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(52.0259, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(312.9500, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(354.5013, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(345.9091, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(260.5897, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(287.3191, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(313.3801, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(420.8497, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(244.8841, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(435.2581, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(346.3680, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(301.3330, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(325.2506, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(358.9459, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(360.7561, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(379.3594, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(306.1015, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(384.6575, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(303.6424, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(334.7605, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(227.1961, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(343.4610, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(322.4342, device='cuda:0', grad_fn=<DivBackward0>)
Loss: [lotus-sys-gpu-33-17] 2025-08-18 10:53:27,658 (trainer:811) INFO: 1epoch:train:701-800batch: iter_time=1.306e-04, forward_time=0.314, loss_ctc=286.587, loss_att=232.402, acc=0.099, loss=259.494, backward_time=2.345, grad_norm=152.233, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.003, optim0_lr0=7.515e-05, train_time=2.698
tensor(306.7589, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(308.1027, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(340.9971, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(290.5588, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(349.7094, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(326.6979, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(339.3628, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(377.2571, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(267.5655, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(355.7569, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(327.0161, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(360.2657, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(306.5491, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(221.0086, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(378.7449, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(223.2527, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(307.3323, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(271.9927, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(135.8053, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(309.6746, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(357.0339, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(313.5935, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(273.5703, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(365.6534, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(341.3352, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(247.3006, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(353.8112, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(231.2828, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(323.6464, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(308.7557, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(197.9742, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(402.0683, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(177.3734, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(375.3922, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(328.4572, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(81.6195, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(287.9510, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(341.4491, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(284.9382, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(216.8551, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(120.1248, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(161.2214, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(317.2956, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(303.4943, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(302.5068, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(264.4582, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(146.3170, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(321.0997, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(362.2655, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(143.1827, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(279.3891, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(350.1573, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(285.2347, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(333.5393, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(255.7610, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(281.2784, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(333.6778, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(303.4891, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(210.6538, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(333.6187, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(349.6312, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(371.2667, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(377.0076, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(351.6589, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(360.2347, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(172.6992, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(375.7140, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(187.1213, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(343.7099, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(334.9440, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(274.3456, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(415.6837, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(266.7860, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(306.7229, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(270.4194, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(251.5632, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(268.7733, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(315.3911, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(467.2735, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(145.0079, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(267.6355, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(264.6199, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(269.3428, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(384.6524, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(297.2142, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(101.6660, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(152.8728, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(386.8593, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(289.5298, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(369.3378, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(187.4894, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(342.5789, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(266.2783, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(259.3588, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(216.7137, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(230.4361, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(375.6973, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(438.7065, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(304.6481, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(314.4300, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(158.1001, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(274.1882, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(317.1491, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(351.6105, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(273.6724, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(288.2429, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(339.7434, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(345.9665, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(377.2721, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(377.6355, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(402.4691, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(330.1082, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(389.6823, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(330.2665, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(333.6975, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(318.2731, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(229.9814, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(330.0903, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(374.0693, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(254.5567, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(353.8289, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(417.1389, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(234.3578, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(265.8525, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(86.2371, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(190.3436, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(247.1757, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(298.7761, device='cuda:0', grad_fn=<DivBackward0>)
Loss: Traceback (most recent call last):
  File "/home/asr/.conda/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/asr/.conda/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/espnet/espnet2/tasks/abs_task.py", line 1227, in main
    cls.main_worker(args)
  File "/espnet/espnet2/tasks/abs_task.py", line 1583, in main_worker
    cls.trainer.run(
  File "/espnet/espnet2/train/trainer.py", line 343, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/espnet/espnet2/train/trainer.py", line 708, in train_one_epoch
    scaler.scale(loss).backward()
  File "/home/asr/.conda/envs/espnet/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/asr/.conda/envs/espnet/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/asr/.conda/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
tensor(372.0249, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(189.9459, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(280.0447, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(314.1809, device='cuda:0', grad_fn=<DivBackward0>)
Loss: tensor(403.4083, device='cuda:0', grad_fn=<DivBackward0>)
