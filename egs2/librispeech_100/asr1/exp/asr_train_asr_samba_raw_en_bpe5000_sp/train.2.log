# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,kaldi_ark --valid_shape_file exp/asr_stats_raw_en_bpe5000_sp/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_samba_raw_en_bpe5000_sp --config conf/tuning/train_asr_samba.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000_sp/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/wav.scp,speech,kaldi_ark --train_shape_file exp/asr_stats_raw_en_bpe5000_sp/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000_sp/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000_sp/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Mon Aug 18 10:13:20 UTC 2025
#
/home/asr/.conda/envs/espnet/bin/python3 /espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,kaldi_ark --valid_shape_file exp/asr_stats_raw_en_bpe5000_sp/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_samba_raw_en_bpe5000_sp --config conf/tuning/train_asr_samba.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000_sp/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/wav.scp,speech,kaldi_ark --train_shape_file exp/asr_stats_raw_en_bpe5000_sp/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000_sp/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000_sp/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[lotus-sys-gpu-33-17] 2025-08-18 10:13:32,072 (asr:540) INFO: Vocabulary size: 5000
[lotus-sys-gpu-33-17] 2025-08-18 10:13:33,386 (abs_task:1400) INFO: pytorch.version=2.6.0+cu126, cuda.available=True, cudnn.version=90501, cudnn.benchmark=True, cudnn.deterministic=False
[lotus-sys-gpu-33-17] 2025-08-18 10:13:33,389 (abs_task:1401) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 20], num_mask=1, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=1, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000_sp/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): SambaASREncoder(
    (embed): Conv2dSubsamplingForSamba(
      (conv): Sequential(
        (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (3): ReLU()
      )
      (out): Linear(in_features=10240, out_features=512, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (pos_enc): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (mamba_blocks): ModuleList(
      (0-5): 6 x MambaBlock(
        (in_proj): Linear(in_features=512, out_features=2048, bias=False)
        (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
        (act): SiLU()
        (x_proj): Linear(in_features=1024, out_features=64, bias=False)
        (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=512, bias=False)
      )
    )
    (layer_norms): ModuleList(
      (0-5): 6 x LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    )
    (final_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): SambaASRDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 512)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (decoders): ModuleList(
      (0-3): 4 x MambaCrossBlock(
        (self_mamba): MambaBlock(
          (in_proj): Linear(in_features=512, out_features=2048, bias=False)
          (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
          (act): SiLU()
          (x_proj): Linear(in_features=1024, out_features=64, bias=False)
          (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=512, bias=False)
        )
        (cross_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (cross_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (ffn): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (output_layer): Linear(in_features=512, out_features=5000, bias=True)
    (after_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=512, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 44.87 M
    Number of trainable parameters: 44.87 M (100.0%)
    Size: 179.47 MB
    Type: torch.float32
[lotus-sys-gpu-33-17] 2025-08-18 10:13:33,389 (abs_task:1404) INFO: Optimizer:
AdamW (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.98]
    capturable: False
    differentiable: False
    eps: 1e-06
    foreach: None
    fused: None
    initial_lr: 0.0002
    lr: 1.0000000000000001e-07
    maximize: False
    weight_decay: 0.01
)
[lotus-sys-gpu-33-17] 2025-08-18 10:13:33,389 (abs_task:1405) INFO: Scheduler: WarmupLR(warmup_steps=2000)
[lotus-sys-gpu-33-17] 2025-08-18 10:13:33,389 (abs_task:1414) INFO: Saving the configuration in exp/asr_train_asr_samba_raw_en_bpe5000_sp/config.yaml
[lotus-sys-gpu-33-17] 2025-08-18 10:13:34,527 (abs_task:1828) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=3630, batch_bins=10000000, sort_in_batch=descending, sort_batch=descending)
[lotus-sys-gpu-33-17] 2025-08-18 10:13:34,528 (abs_task:1829) INFO: [train] mini-batch sizes summary: N-batch=3630, mean=23.6, min=6, max=122
[lotus-sys-gpu-33-17] 2025-08-18 10:13:34,594 (read_text:31) INFO: keys_to_load is not None, only loading 85617 keys from dump/raw/train_clean_100_sp/text
[lotus-sys-gpu-33-17] 2025-08-18 10:13:34,686 (asr:512) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lotus-sys-gpu-33-17] 2025-08-18 10:13:34,688 (abs_task:1853) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train_clean_100_sp/wav.scp", "type": "kaldi_ark"}
  text: {"path": "dump/raw/train_clean_100_sp/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7fd81edae0b0>)
[lotus-sys-gpu-33-17] 2025-08-18 10:13:34,722 (abs_task:1828) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=136, batch_bins=10000000, sort_in_batch=descending, sort_batch=descending)
[lotus-sys-gpu-33-17] 2025-08-18 10:13:34,722 (abs_task:1829) INFO: [valid] mini-batch sizes summary: N-batch=136, mean=40.8, min=3, max=125
[lotus-sys-gpu-33-17] 2025-08-18 10:13:34,726 (read_text:31) INFO: keys_to_load is not None, only loading 5551 keys from dump/raw/dev/text
[lotus-sys-gpu-33-17] 2025-08-18 10:13:34,731 (asr:512) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lotus-sys-gpu-33-17] 2025-08-18 10:13:34,731 (abs_task:1853) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "kaldi_ark"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7fd81ec14130>)
[lotus-sys-gpu-33-17] 2025-08-18 10:13:34,738 (abs_task:1828) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=5551, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000_sp/valid/speech_shape, 
[lotus-sys-gpu-33-17] 2025-08-18 10:13:34,739 (abs_task:1829) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[lotus-sys-gpu-33-17] 2025-08-18 10:13:34,742 (read_text:31) INFO: keys_to_load is not None, only loading 3 keys from dump/raw/dev/text
[lotus-sys-gpu-33-17] 2025-08-18 10:13:34,746 (asr:512) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lotus-sys-gpu-33-17] 2025-08-18 10:13:34,746 (abs_task:1853) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "kaldi_ark"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7fd81ec16020>)
/espnet/espnet2/train/trainer.py:219: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
[lotus-sys-gpu-33-17] 2025-08-18 10:13:35,017 (trainer:336) INFO: 1/50epoch started
Failed to import Flash Attention, using ESPnet default: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /home/asr/.conda/envs/espnet/lib/python3.10/site-packages/flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so)
/espnet/espnet2/train/trainer.py:638: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(
/espnet/espnet2/asr/espnet_model.py:402: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(self.autocast_frontend, dtype=autocast_type):
Traceback (most recent call last):
  File "/home/asr/.conda/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/asr/.conda/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/espnet/espnet2/tasks/abs_task.py", line 1227, in main
    cls.main_worker(args)
  File "/espnet/espnet2/tasks/abs_task.py", line 1583, in main_worker
    cls.trainer.run(
  File "/espnet/espnet2/train/trainer.py", line 343, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/espnet/espnet2/train/trainer.py", line 708, in train_one_epoch
    scaler.scale(loss).backward()
  File "/home/asr/.conda/envs/espnet/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/asr/.conda/envs/espnet/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/asr/.conda/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 3.81 MiB is free. Process 2714434 has 23.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 52.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error in Mamba block 5: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 5.81 MiB is free. Process 2714434 has 23.67 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 2.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Loss: tensor(3209.8552, device='cuda:0', grad_fn=<DivBackward0>)
# Accounting: time=24 threads=1
# Ended (code 1) at Mon Aug 18 10:13:44 UTC 2025, elapsed time 24 seconds
